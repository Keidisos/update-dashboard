# SOC System - Setup Guide

## üöÄ D√©marrage rapide

### 1. Lancer Ollama avec le mod√®le CyberLLM

```bash
# 1. D√©marrer les services (avec Ollama)
cd /docker/update-dashboard
docker compose up -d

# 2. V√©rifier qu'Ollama est d√©marr√©
docker ps | grep ollama

# 3. T√©l√©charger le mod√®le de base llama3.1:8b
docker exec -it update-dashboard-ollama ollama pull llama3.1:8b

# 4. Cr√©er le mod√®le custom CyberLLM
docker exec -it update-dashboard-ollama ollama create cybersec -f /app/ollama/Modelfile_cybersec

# 5. V√©rifiern que le mod√®le est cr√©√©
docker exec -it update-dashboard-ollama ollama list
```

Vous devriez voir `cybersec` dans la liste des mod√®les !

---

## üîç Tester le SOC

### V√©rifier la connexion Ollama

```bash
curl http://localhost:8080/api/v1/soc/health
```

### Lancer une analyse manuelle

```bash
# via curl
curl -X POST http://localhost:8080/api/v1/soc/analyze/1

# via UI
# Allez sur http://localhost:8080 et naviguez vers SOC
```

### Voir les incidents d√©tect√©s

```bash
curl http://localhost:8080/api/v1/soc/incidents
```

### Obtenir les statistiques

```bash
curl http://localhost:8080/api/v1/soc/stats
```

---

## üéØ Ce qui est analys√©

Le SOC collecte et analyse :

- ‚úÖ **auth.log** : Tentatives de connexion SSH
- ‚úÖ D√©tection de **brute-force**
- ‚úÖ Connexions depuis IPs inhabituelles
- ‚úÖ Escalades de privil√®ges
- ‚úÖ Commandes sudo suspectes

## üìä Donn√©es renvoy√©es par l'IA

L'IA CyberLLM retourne pour chaque analyse :

- **Severity** : low, medium, high, critical
- **Threat type** : brute_force, ssh_intrusion, etc.
- **Description** d√©taill√©e technique
- **Recommendations** de rem√©diation
- **MITRE ATT&CK** techniques (T1078, T1110, etc.)
- **IPs sources** suspectes
- **Utilisateurs** affect√©s

---

## ‚öôÔ∏è Configuration

Dans `.env` :

```env
# SOC Configuration
SOC_ENABLED=true
SOC_ANALYSIS_INTERVAL=10     # minutes entre chaque analyse auto
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=cybersec
```

---

## üêõ Troubleshooting

### Ollama ne r√©pond pas

```bash
# V√©rifier les logs
docker logs update-dashboard-ollama

# Red√©marrer
docker compose restart ollama
```

### Le mod√®le n'existe pas

```bash
# Recr√©er le mod√®le
docker exec -it update-dashboard-ollama ollama create cybersec -f /app/ollama/Modelfile_cybersec
```

### Pas assez de RAM

Le mod√®le llama3.1:8b n√©cessite ~8GB de RAM. Si vous n'avez pas assez :

```bash
# Utiliser un mod√®le plus l√©ger
docker exec -it update-dashboard-ollama ollama pull llama3.2

# Modifier le Modelfile pour utiliser llama3.2 au lieu de llama3.1:8b
```

---

## üé® Prochaines √©tapes

- [ ] Page dashboard SOC UI (Phase 1 complete)
- [ ] Collecte logs containers Docker
- [ ] Analyse de logs multiples sources
- [ ] Corr√©lation d'√©v√©nements
- [ ] Notifications Discord pour incidents critiques
